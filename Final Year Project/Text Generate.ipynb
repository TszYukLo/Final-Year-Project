{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83ca0807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import tokenization\n",
    "from tokenization import tokenize_word\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from data_collection import collect_conversation\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "import data_collection\n",
    "import tokenization\n",
    "import train_model\n",
    "from train_model import plot_model_accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4bed086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train response or not model\n",
      "Metal device set to: Apple M1 Pro\n",
      "8000\n",
      "(8000, 25)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-29 20:29:47.833367: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-04-29 20:29:47.833510: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 25, 8)             10832     \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 256)              140288    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 169,681\n",
      "Trainable params: 169,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-29 20:29:48.281483: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-04-29 20:29:49.560352: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-29 20:29:49.844821: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-29 20:29:49.857307: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-29 20:29:50.870443: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-29 20:29:50.891458: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m response_train \u001b[38;5;241m=\u001b[39m [padded_tokenized_user_response_train, user_input_response_train_label]\n\u001b[1;32m     46\u001b[0m response_test \u001b[38;5;241m=\u001b[39m [padded_tokenized_user_response_test, user_input_response_test_label]\n\u001b[0;32m---> 47\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_response_or_not_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer_for_response\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# stage check model (using functional structure)\u001b[39;00m\n\u001b[1;32m     51\u001b[0m stage_user_message, stage_system_message, stage_label \u001b[38;5;241m=\u001b[39m [], [], []\n",
      "File \u001b[0;32m~/Desktop/FinalYearProject/train_model.py:202\u001b[0m, in \u001b[0;36mtrain_response_or_not_models\u001b[0;34m(tokenizer, response_or_not_model_train, response_or_not_model_test)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28mprint\u001b[39m(response_or_not_model\u001b[38;5;241m.\u001b[39msummary())\n\u001b[1;32m    200\u001b[0m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mplot_model(response_or_not_model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse_or_not_model.png\u001b[39m\u001b[38;5;124m'\u001b[39m, show_shapes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 202\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mresponse_or_not_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_message_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_label_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresponse_message_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_label_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m                                \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m plot_model_accurate(history)    \u001b[38;5;66;03m# plot graph\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;66;03m# response_or_not_model.save('response_or_not.h5')\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \n\u001b[1;32m    208\u001b[0m \u001b[38;5;66;03m# test the model    0 = not reply, 1 = reply\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/keras/engine/training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1378\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1379\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   1380\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m   1381\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1382\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1383\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1384\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1385\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1386\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2953\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2954\u001b[0m   (graph_function,\n\u001b[1;32m   2955\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2956\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1849\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1851\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1852\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1853\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1854\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1855\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m     args,\n\u001b[1;32m   1857\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1858\u001b[0m     executing_eagerly)\n\u001b[1;32m   1859\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "text_generation_dataset = collect_conversation()    # collect dataset\n",
    "dataset_for_hotel = text_generation_dataset[0]\n",
    "dataset_for_restaurant = text_generation_dataset[1]\n",
    "\n",
    "# data collect and setting part\n",
    "# label the message need to response or not\n",
    "# if system response is '', then the label will be 0 (means no need to response)\n",
    "user_input_message, system_output_message = [], []\n",
    "user_input_response_label, turn_stage_with_messages = [], []\n",
    "for conversation in dataset_for_hotel:\n",
    "    user_input_message.append(conversation['hotel_user_transcript'])\n",
    "    system_output_message.append(conversation['hotel_system_transcript'])\n",
    "    turn_stage_with_messages.append({\"turn_id\": conversation['hotel_turn_id'],\n",
    "                                     \"user_input\": conversation['hotel_user_transcript'],\n",
    "                                     \"system_output\": conversation['hotel_system_transcript']\n",
    "                                     })\n",
    "    if conversation['hotel_system_transcript'] == '':\n",
    "        user_input_response_label.append(0)     # 0 means no need to response\n",
    "    else:\n",
    "        user_input_response_label.append(1)     # 1 means need to response\n",
    "for conversation in dataset_for_restaurant:\n",
    "    user_input_message.append(conversation['restaurant_user_transcript'])\n",
    "    system_output_message.append(conversation['restaurant_system_transcript'])\n",
    "    turn_stage_with_messages.append({\"turn_id\": conversation['restaurant_turn_id'],\n",
    "                                     \"user_input\": conversation['restaurant_user_transcript'],\n",
    "                                     \"system_output\": conversation['restaurant_system_transcript']\n",
    "                                     })\n",
    "    if conversation['restaurant_system_transcript'] == '':\n",
    "        user_input_response_label.append(0)     # 0 means no need to response\n",
    "    else:\n",
    "        user_input_response_label.append(1)     # 1 means need to response\n",
    "\n",
    "\n",
    "# split data for train and test (train response or not)\n",
    "user_input_response_train = user_input_message[:8000]\n",
    "user_input_response_train_label = user_input_response_label[:8000]\n",
    "user_input_response_test = user_input_message[8000:]\n",
    "user_input_response_test_label = user_input_response_label[8000:]\n",
    "\n",
    "# Dictionary    !!! Tokenizer !!!\n",
    "tokenizer_for_response, padded_tokenized_user_response_train, padded_tokenized_user_response_test \\\n",
    "    = tokenization.tokenize_message(user_input_response_train, user_input_response_test)\n",
    "# train model\n",
    "response_train = [padded_tokenized_user_response_train, user_input_response_train_label]\n",
    "response_test = [padded_tokenized_user_response_test, user_input_response_test_label]\n",
    "train_model.train_response_or_not_models(tokenizer_for_response, response_train, response_test)\n",
    "\n",
    "\n",
    "# stage check model (using functional structure)\n",
    "stage_user_message, stage_system_message, stage_label = [], [], []\n",
    "for turn in turn_stage_with_messages:\n",
    "    stage_user_message.append(turn['user_input'])\n",
    "    stage_system_message.append(turn['system_output'])\n",
    "    if turn['turn_id'] == 0:\n",
    "        turn_stage_label = [1, 0, 0]\n",
    "    elif turn['system_output'] == '':\n",
    "        turn_stage_label = [0, 0, 1]\n",
    "    else:\n",
    "        turn_stage_label = [0, 1, 0]\n",
    "    stage_label.append(turn_stage_label)\n",
    "\n",
    "user_input_stage_train = stage_user_message[:8000]\n",
    "user_input_stage_train_label = stage_label[:8000]\n",
    "user_input_stage_test = stage_user_message[8000:]\n",
    "user_input_stage_test_label = stage_label[8000:]\n",
    "\n",
    "# add 'start=(index3)' and 'end'=(index2) in the corresponding position\n",
    "stage_system_message_with_tag = []\n",
    "for sentence in stage_system_message:\n",
    "    if sentence != '':\n",
    "        stage_system_message_with_tag.append('start ' + sentence + ' end') # no /start, /end since not in all sentence\n",
    "system_input_stage_train = stage_system_message_with_tag[:6000]\n",
    "system_input_stage_test = stage_system_message_with_tag[6000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4341e4f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2 Dictionary\n",
    "# user dictionary\n",
    "tokenizer_for_stage_for_user, padded_tokenized_user_stage_train, padded_tokenized_user_stage_test \\\n",
    "    = tokenization.tokenize_message(user_input_stage_train, user_input_stage_test)\n",
    "tokenizer_for_stage_for_system, padded_tokenized_system_stage_train, padded_tokenized_system_stage_test \\\n",
    "    = tokenization.tokenize_message_for_response(system_input_stage_train, system_input_stage_test)\n",
    "\n",
    "# train model stage   (check message need to response or not)\n",
    "stage_train = [padded_tokenized_user_stage_train, user_input_stage_train_label, user_input_response_train_label]\n",
    "stage_test = [padded_tokenized_user_stage_test, user_input_stage_test_label, user_input_response_test_label]\n",
    "train_model.message_stage_model(tokenizer_for_stage_for_user, stage_train, stage_test)\n",
    "\n",
    "# train response model\n",
    "# for system\n",
    "all_sentence_predict_pair, user_input_index = [], 0\n",
    "for sentence in system_input_stage_train:\n",
    "    all_round_message, previous_message = [], ''\n",
    "    sentence_in_list = sentence.split()\n",
    "    for word in sentence_in_list:\n",
    "        if word == 0:\n",
    "            break\n",
    "        tokenized_transcript = tokenizer_for_stage_for_system.texts_to_sequences([previous_message])\n",
    "        tokenized_transcript_word = tokenizer_for_stage_for_system.texts_to_sequences([word])\n",
    "        padded_tokenized_transcript = pad_sequences(tokenized_transcript, maxlen=35, padding='post', truncating='post')\n",
    "        padded_tokenized_keyword = pad_sequences(tokenized_transcript_word, maxlen=1, padding='post', truncating='post')\n",
    "        all_round_message.append({'user_input': padded_tokenized_user_stage_train[user_input_index],\n",
    "                                  'message': padded_tokenized_transcript[0], 'predict_word': tokenization.one_hot(padded_tokenized_keyword[0])})\n",
    "        previous_message = previous_message + word + ' '\n",
    "    user_input_index += 1\n",
    "    all_sentence_predict_pair.append(all_round_message)\n",
    "user_input_index = 0\n",
    "for sentence in system_input_stage_test:\n",
    "    all_round_message, previous_message = [], ''\n",
    "    sentence_in_list = sentence.split()\n",
    "    for word in sentence_in_list:\n",
    "        if word == 0:\n",
    "            break\n",
    "        tokenized_transcript = tokenizer_for_stage_for_system.texts_to_sequences([previous_message])\n",
    "        tokenized_transcript_word = tokenizer_for_stage_for_system.texts_to_sequences([word])\n",
    "        padded_tokenized_transcript = pad_sequences(tokenized_transcript, maxlen=35, padding='post', truncating='post')\n",
    "        padded_tokenized_keyword = pad_sequences(tokenized_transcript_word, maxlen=1, padding='post', truncating='post')\n",
    "        \n",
    "        all_round_message.append({'user_input': padded_tokenized_user_stage_test[user_input_index],\n",
    "                                  'message': padded_tokenized_transcript[0], 'predict_word': tokenization.one_hot(padded_tokenized_keyword[0])})\n",
    "        previous_message = previous_message + word + ' '\n",
    "    all_sentence_predict_pair.append(all_round_message)\n",
    "print(len(all_sentence_predict_pair))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c44b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all pair as array\n",
    "data_dic_train = all_sentence_predict_pair[:4700]\n",
    "data_dic_test = all_sentence_predict_pair[4700:]\n",
    "user_input_pair_train, system_message_train, predict_word_train = [], [], []\n",
    "for sentence in data_dic_train:\n",
    "    for turn in sentence:\n",
    "        user_input_pair_train.append(turn['user_input'])\n",
    "        system_message_train.append(turn['message'])\n",
    "        predict_word_train.append(turn['predict_word'])\n",
    "user_input_pair_test, system_message_test, predict_word_test = [], [], []\n",
    "for sentence in data_dic_test:\n",
    "    for turn in sentence:\n",
    "        user_input_pair_test.append(turn['user_input'])\n",
    "        system_message_test.append(turn['message'])\n",
    "        predict_word_test.append(turn['predict_word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69b4631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "# size: user_input = 25, generated_input = 35, output_probobility = 2514\n",
    "print(\"Train message generate model\")\n",
    "user_message_input = tf.keras.Input(shape=(25,), name=\"user_input\")\n",
    "system_accrue_message_input = tf.keras.Input(shape=(35,), name=\"system_accrue_message_input\")\n",
    "user_message_features = tf.keras.layers.Embedding(3000+1,32, input_length=25)(user_message_input)\n",
    "system_message_features = tf.keras.layers.Embedding(700+1, 32, input_length=35)(system_accrue_message_input)\n",
    "\n",
    "user_message_features = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32))(user_message_features)\n",
    "system_message_features = tf.keras.layers.LSTM(64)(system_message_features)\n",
    "combine_layer = tf.keras.layers.concatenate([user_message_features, system_message_features])\n",
    "control_range_layer = tf.keras.layers.Dense(32, activation='linear')(combine_layer)\n",
    "control_range_layer = tf.keras.layers.Dense(16, activation='relu')(control_range_layer)\n",
    "predict_output = tf.keras.layers.Dense(700, name=\"predict_output\", activation='sigmoid')(control_range_layer)\n",
    "response_model = tf.keras.Model(inputs=[user_message_input, system_accrue_message_input],\n",
    "                                outputs=[predict_output], )\n",
    "response_model.compile(loss=['binary_crossentropy'], optimizer='adam', metrics=['acc'])\n",
    "print(response_model.summary())\n",
    "# tf.keras.utils.plot_model(response_model, 'stage_model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4a0890",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# # problem start at here\n",
    "history = response_model.fit({\"user_input\": np.array(user_input_pair_train), \"system_accrue_message_input\": np.array(system_message_train)},\n",
    "                             np.array(predict_word_train), epochs=35,\n",
    "                             validation_data=({\"user_input\": np.array(user_input_pair_test), \"system_accrue_message_input\": np.array(system_message_test)},\n",
    "                                       np.array(predict_word_test)), verbose=2)\n",
    "plot_model_accurate(history)    # plot graph\n",
    "response_model.save('system_response.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8657effb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model\n",
    "user_input = [ 'I would like to book a cheap hotel'\n",
    "              ,\"no, i just need to make sure it's cheap.\"\n",
    "              ]\n",
    "user_input_sentence = tokenizer_for_stage_for_user.texts_to_sequences(user_input)\n",
    "system_output = [''\n",
    "                 ,\"start i found 1 hotel in the north would you like me to book it for \"\n",
    "                 ]\n",
    "system_output_sentence = tokenizer_for_stage_for_system.texts_to_sequences(system_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d1f91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input layer data text\n",
    "padded_user_input = pad_sequences(user_input_sentence, maxlen=25, padding='post', truncating='post')\n",
    "padded_predicted_sentence = pad_sequences(system_output_sentence, maxlen=35, padding='post', truncating='post')\n",
    "print(padded_user_input)\n",
    "print(padded_predicted_sentence)\n",
    "all_prediction = response_model.predict({\"user_input\": padded_user_input, \"system_accrue_message_input\": padded_predicted_sentence})\n",
    "\n",
    "print(len(all_prediction))\n",
    "for predict in all_prediction:\n",
    "    print(max(predict))\n",
    "    for index in range(len(predict)):\n",
    "        if predict[index] == max(predict):\n",
    "            print(index)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4af799c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer_for_stage_for_system.word_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
